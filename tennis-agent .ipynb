{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the model\n",
    "\n",
    "in this project we will try to solve the probelm with DDPG network, so wee need to difine 2 networks the actor and the critic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class actor_network(nn.Module):\n",
    "    def __init__(self,state_size = 24, action_size =2 ,seed = 0):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size,120)\n",
    "        self.fc2 = nn.Linear(120,128)   \n",
    "        self.fc3 = nn.Linear(128,130)   \n",
    "        self.output = nn.Linear(130,action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        \n",
    "        \n",
    "        return F.tanh(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_network(nn.Module):\n",
    "    def __init__(self,state_size = 24,action_size = 2 ,seed = 0):\n",
    "        super(critic_network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size,120)\n",
    "        self.fc2 = nn.Linear(120 + action_size,128)\n",
    "        self.fc3 = nn.Linear(128,130)\n",
    "        self.output = nn.Linear(130,1)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, actions):\n",
    "        x = F.leaky_relu(self.fc1(state))\n",
    "        x = torch.cat((x,actions),dim = 1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR_ACTOR = 1e-5         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "LEARN_EVERY = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agents():\n",
    " \n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.steps = 0\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = actor_network(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = actor_network(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = critic_network(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = critic_network(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "   \n",
    "            \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experiences one by one in the memory before the learning operation\n",
    "        for i in range(self.num_agents):\n",
    "            self.memory.add(state[i,:], action[i,:], reward[i], next_state[i,:], done[i])\n",
    "\n",
    "        self.steps +=1\n",
    "        if self.steps % LEARN_EVERY == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                for _ in range(3):\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = np.zeros((self.num_agents, self.action_size))\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            for agent_num, state in enumerate(states):\n",
    "                action = self.actor_local(state).cpu().data.numpy()\n",
    "                actions[agent_num, :] = action\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            actions += self.noise.sample()\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "      \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # clip the grad to avoid its explosing\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # clip the grad to avoid its explosing\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor_local.parameters(), 1)\n",
    "          \n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tring the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agents(state_size=24, action_size=2,num_agents=2, random_seed=0)\n",
    "\n",
    "\n",
    "\n",
    "def train_DDPG(agent,episodes=2000,number_agents = 2,max_t = 1000):\n",
    "    best_mean_score = -100\n",
    "    scores_array = []\n",
    "    mean_scores = []\n",
    "    max_scores = []\n",
    "    min_scores = []\n",
    "    scores_window = deque(maxlen = 100)\n",
    "   \n",
    "    \n",
    "    for episode in range(1,episodes+1):\n",
    "        states = env.reset(train_mode = True)[brain_name].vector_observations\n",
    "        scores = np.zeros(number_agents)\n",
    "        agent.reset()\n",
    "        for i in range(max_t):\n",
    "            actions = agent.act(states,add_noise = True)\n",
    "            step_info = env.step(actions)[brain_name]\n",
    "            next_states = step_info.vector_observations\n",
    "            rewards = step_info.rewards\n",
    "            dones = step_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        mean_scores.append(np.mean(scores))\n",
    "        max_scores.append(np.max(scores))\n",
    "        min_scores.append(np.min(scores))\n",
    "        scores_window.append(mean_scores[-1])\n",
    "        scores_array.append(scores)\n",
    "        \n",
    "        print('\\rEpisode {}\\t Average Score: {:.2f}\\t max score: {:.2f}\\t min score: {:.2f}'.format(episode,np.mean(scores_window),max_scores[-1],min_scores[-1]), end=\"\")\n",
    "        if episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "        \n",
    "        \n",
    "        if np.mean(scores_window)> best_mean_score:\n",
    "            best_mean_score = np.mean(scores_window)\n",
    "            print('\\nEnvironment saved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_local_checkpointV2.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_local_checkpointV2.pth')\n",
    "            \n",
    "            \n",
    "    return scores_window,scores_array,mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\t Average Score: -0.00\t max score: 0.00\t min score: -0.01\n",
      "Environment saved in 1 episodes!\tAverage Score: -0.00\n",
      "Episode 8\t Average Score: 0.00\t max score: 0.10\t min score: -0.011\n",
      "Environment saved in 8 episodes!\tAverage Score: 0.00\n",
      "Episode 50\tAverage Score: -0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 100\tAverage Score: -0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 150\tAverage Score: -0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 200\tAverage Score: -0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 250\tAverage Score: -0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 292\t Average Score: 0.00\t max score: 0.10\t min score: 0.0911\n",
      "Environment saved in 292 episodes!\tAverage Score: 0.00\n",
      "Episode 293\t Average Score: 0.00\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 293 episodes!\tAverage Score: 0.00\n",
      "Episode 297\t Average Score: 0.00\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 297 episodes!\tAverage Score: 0.00\n",
      "Episode 300\tAverage Score: 0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 303\t Average Score: 0.00\t max score: 0.09\t min score: 0.001\n",
      "Environment saved in 303 episodes!\tAverage Score: 0.00\n",
      "Episode 306\t Average Score: 0.00\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 306 episodes!\tAverage Score: 0.00\n",
      "Episode 315\t Average Score: 0.01\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 315 episodes!\tAverage Score: 0.01\n",
      "Episode 329\t Average Score: 0.01\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 329 episodes!\tAverage Score: 0.01\n",
      "Episode 350\tAverage Score: 0.011\t max score: 0.00\t min score: -0.01\n",
      "Episode 352\t Average Score: 0.01\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 352 episodes!\tAverage Score: 0.01\n",
      "Episode 353\t Average Score: 0.01\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 353 episodes!\tAverage Score: 0.01\n",
      "Episode 355\t Average Score: 0.01\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 355 episodes!\tAverage Score: 0.01\n",
      "Episode 359\t Average Score: 0.01\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 359 episodes!\tAverage Score: 0.01\n",
      "Episode 400\tAverage Score: 0.011\t max score: 0.00\t min score: -0.01\n",
      "Episode 450\tAverage Score: 0.000\t max score: 0.00\t min score: -0.01\n",
      "Episode 500\tAverage Score: 0.000\t max score: 0.00\t min score: -0.011\n",
      "Episode 528\t Average Score: 0.01\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 528 episodes!\tAverage Score: 0.01\n",
      "Episode 534\t Average Score: 0.01\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 534 episodes!\tAverage Score: 0.01\n",
      "Episode 535\t Average Score: 0.01\t max score: 0.09\t min score: 0.00\n",
      "Environment saved in 535 episodes!\tAverage Score: 0.01\n",
      "Episode 540\t Average Score: 0.01\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 540 episodes!\tAverage Score: 0.01\n",
      "Episode 541\t Average Score: 0.01\t max score: 0.09\t min score: 0.00\n",
      "Environment saved in 541 episodes!\tAverage Score: 0.01\n",
      "Episode 542\t Average Score: 0.01\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 542 episodes!\tAverage Score: 0.01\n",
      "Episode 546\t Average Score: 0.01\t max score: 0.09\t min score: 0.001\n",
      "Environment saved in 546 episodes!\tAverage Score: 0.01\n",
      "Episode 547\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 547 episodes!\tAverage Score: 0.02\n",
      "Episode 550\tAverage Score: 0.022\t max score: 0.10\t min score: -0.01\n",
      "\n",
      "Environment saved in 550 episodes!\tAverage Score: 0.02\n",
      "Episode 552\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 552 episodes!\tAverage Score: 0.02\n",
      "Episode 555\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 555 episodes!\tAverage Score: 0.02\n",
      "Episode 556\t Average Score: 0.02\t max score: 0.09\t min score: 0.00\n",
      "Environment saved in 556 episodes!\tAverage Score: 0.02\n",
      "Episode 558\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 558 episodes!\tAverage Score: 0.02\n",
      "Episode 568\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 568 episodes!\tAverage Score: 0.02\n",
      "Episode 571\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 571 episodes!\tAverage Score: 0.02\n",
      "Episode 572\t Average Score: 0.02\t max score: 0.09\t min score: 0.00\n",
      "Environment saved in 572 episodes!\tAverage Score: 0.02\n",
      "Episode 573\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 573 episodes!\tAverage Score: 0.02\n",
      "Episode 575\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 575 episodes!\tAverage Score: 0.02\n",
      "Episode 578\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 578 episodes!\tAverage Score: 0.02\n",
      "Episode 580\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 580 episodes!\tAverage Score: 0.02\n",
      "Episode 581\t Average Score: 0.02\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 581 episodes!\tAverage Score: 0.02\n",
      "Episode 584\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 584 episodes!\tAverage Score: 0.02\n",
      "Episode 586\t Average Score: 0.02\t max score: 0.09\t min score: 0.001\n",
      "Environment saved in 586 episodes!\tAverage Score: 0.02\n",
      "Episode 592\t Average Score: 0.02\t max score: 0.09\t min score: 0.001\n",
      "Environment saved in 592 episodes!\tAverage Score: 0.02\n",
      "Episode 600\tAverage Score: 0.022\t max score: 0.00\t min score: -0.01\n",
      "Episode 605\t Average Score: 0.02\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 605 episodes!\tAverage Score: 0.02\n",
      "Episode 612\t Average Score: 0.02\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 612 episodes!\tAverage Score: 0.02\n",
      "Episode 613\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 613 episodes!\tAverage Score: 0.03\n",
      "Episode 615\t Average Score: 0.03\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 615 episodes!\tAverage Score: 0.03\n",
      "Episode 650\tAverage Score: 0.022\t max score: 0.10\t min score: 0.091\n",
      "Episode 700\tAverage Score: 0.033\t max score: 0.10\t min score: -0.01\n",
      "Episode 725\t Average Score: 0.03\t max score: 0.09\t min score: 0.001\n",
      "Environment saved in 725 episodes!\tAverage Score: 0.03\n",
      "Episode 726\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 726 episodes!\tAverage Score: 0.03\n",
      "Episode 727\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 727 episodes!\tAverage Score: 0.03\n",
      "Episode 728\t Average Score: 0.03\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 728 episodes!\tAverage Score: 0.03\n",
      "Episode 729\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 729 episodes!\tAverage Score: 0.03\n",
      "Episode 730\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 730 episodes!\tAverage Score: 0.03\n",
      "Episode 731\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 731 episodes!\tAverage Score: 0.03\n",
      "Episode 750\tAverage Score: 0.033\t max score: 0.09\t min score: 0.001\n",
      "Episode 757\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 757 episodes!\tAverage Score: 0.03\n",
      "Episode 758\t Average Score: 0.03\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 758 episodes!\tAverage Score: 0.03\n",
      "Episode 759\t Average Score: 0.03\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 759 episodes!\tAverage Score: 0.03\n",
      "Episode 761\t Average Score: 0.04\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 761 episodes!\tAverage Score: 0.04\n",
      "Episode 763\t Average Score: 0.04\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 763 episodes!\tAverage Score: 0.04\n",
      "Episode 765\t Average Score: 0.04\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 765 episodes!\tAverage Score: 0.04\n",
      "Episode 784\t Average Score: 0.04\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 784 episodes!\tAverage Score: 0.04\n",
      "Episode 789\t Average Score: 0.04\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 789 episodes!\tAverage Score: 0.04\n",
      "Episode 790\t Average Score: 0.04\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 790 episodes!\tAverage Score: 0.04\n",
      "Episode 800\tAverage Score: 0.044\t max score: 0.00\t min score: -0.01\n",
      "Episode 803\t Average Score: 0.04\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 803 episodes!\tAverage Score: 0.04\n",
      "Episode 816\t Average Score: 0.04\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 816 episodes!\tAverage Score: 0.04\n",
      "Episode 819\t Average Score: 0.04\t max score: 0.20\t min score: -0.01\n",
      "Environment saved in 819 episodes!\tAverage Score: 0.04\n",
      "Episode 820\t Average Score: 0.04\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 820 episodes!\tAverage Score: 0.04\n",
      "Episode 821\t Average Score: 0.04\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 821 episodes!\tAverage Score: 0.04\n",
      "Episode 850\tAverage Score: 0.044\t max score: 0.00\t min score: -0.01\n",
      "Episode 874\t Average Score: 0.04\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 874 episodes!\tAverage Score: 0.04\n",
      "Episode 876\t Average Score: 0.04\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 876 episodes!\tAverage Score: 0.04\n",
      "Episode 877\t Average Score: 0.04\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 877 episodes!\tAverage Score: 0.04\n",
      "Episode 880\t Average Score: 0.04\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 880 episodes!\tAverage Score: 0.04\n",
      "Episode 896\t Average Score: 0.04\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 896 episodes!\tAverage Score: 0.04\n",
      "Episode 900\tAverage Score: 0.044\t max score: 0.00\t min score: -0.01\n",
      "Episode 924\t Average Score: 0.04\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 924 episodes!\tAverage Score: 0.04\n",
      "Episode 925\t Average Score: 0.05\t max score: 0.30\t min score: 0.29\n",
      "Environment saved in 925 episodes!\tAverage Score: 0.05\n",
      "Episode 926\t Average Score: 0.05\t max score: 0.09\t min score: 0.00\n",
      "Environment saved in 926 episodes!\tAverage Score: 0.05\n",
      "Episode 931\t Average Score: 0.05\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 931 episodes!\tAverage Score: 0.05\n",
      "Episode 932\t Average Score: 0.05\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 932 episodes!\tAverage Score: 0.05\n",
      "Episode 933\t Average Score: 0.05\t max score: 0.30\t min score: 0.29\n",
      "Environment saved in 933 episodes!\tAverage Score: 0.05\n",
      "Episode 935\t Average Score: 0.05\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 935 episodes!\tAverage Score: 0.05\n",
      "Episode 937\t Average Score: 0.05\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 937 episodes!\tAverage Score: 0.05\n",
      "Episode 940\t Average Score: 0.05\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 940 episodes!\tAverage Score: 0.05\n",
      "Episode 950\tAverage Score: 0.055\t max score: 0.10\t min score: -0.01\n",
      "Episode 955\t Average Score: 0.06\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 955 episodes!\tAverage Score: 0.06\n",
      "Episode 976\t Average Score: 0.06\t max score: 0.19\t min score: 0.101\n",
      "Environment saved in 976 episodes!\tAverage Score: 0.06\n",
      "Episode 978\t Average Score: 0.06\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 978 episodes!\tAverage Score: 0.06\n",
      "Episode 979\t Average Score: 0.06\t max score: 0.19\t min score: 0.10\n",
      "Environment saved in 979 episodes!\tAverage Score: 0.06\n",
      "Episode 982\t Average Score: 0.06\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 982 episodes!\tAverage Score: 0.06\n",
      "Episode 984\t Average Score: 0.06\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 984 episodes!\tAverage Score: 0.06\n",
      "Episode 985\t Average Score: 0.06\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 985 episodes!\tAverage Score: 0.06\n",
      "Episode 986\t Average Score: 0.06\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 986 episodes!\tAverage Score: 0.06\n",
      "Episode 988\t Average Score: 0.06\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 988 episodes!\tAverage Score: 0.06\n",
      "Episode 992\t Average Score: 0.06\t max score: 0.09\t min score: 0.001\n",
      "Environment saved in 992 episodes!\tAverage Score: 0.06\n",
      "Episode 993\t Average Score: 0.06\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 993 episodes!\tAverage Score: 0.06\n",
      "Episode 998\t Average Score: 0.06\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 998 episodes!\tAverage Score: 0.06\n",
      "Episode 999\t Average Score: 0.06\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 999 episodes!\tAverage Score: 0.06\n",
      "Episode 1000\tAverage Score: 0.066\t max score: 0.10\t min score: -0.01\n",
      "\n",
      "Environment saved in 1000 episodes!\tAverage Score: 0.06\n",
      "Episode 1001\t Average Score: 0.06\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1001 episodes!\tAverage Score: 0.06\n",
      "Episode 1003\t Average Score: 0.07\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 1003 episodes!\tAverage Score: 0.07\n",
      "Episode 1008\t Average Score: 0.07\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 1008 episodes!\tAverage Score: 0.07\n",
      "Episode 1015\t Average Score: 0.07\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 1015 episodes!\tAverage Score: 0.07\n",
      "Episode 1018\t Average Score: 0.07\t max score: 0.30\t min score: 0.291\n",
      "Environment saved in 1018 episodes!\tAverage Score: 0.07\n",
      "Episode 1020\t Average Score: 0.07\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 1020 episodes!\tAverage Score: 0.07\n",
      "Episode 1028\t Average Score: 0.07\t max score: 0.30\t min score: 0.191\n",
      "Environment saved in 1028 episodes!\tAverage Score: 0.07\n",
      "Episode 1036\t Average Score: 0.07\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 1036 episodes!\tAverage Score: 0.07\n",
      "Episode 1039\t Average Score: 0.07\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 1039 episodes!\tAverage Score: 0.07\n",
      "Episode 1042\t Average Score: 0.08\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 1042 episodes!\tAverage Score: 0.08\n",
      "Episode 1043\t Average Score: 0.08\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1043 episodes!\tAverage Score: 0.08\n",
      "Episode 1044\t Average Score: 0.08\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 1044 episodes!\tAverage Score: 0.08\n",
      "Episode 1045\t Average Score: 0.08\t max score: 0.40\t min score: 0.29\n",
      "Environment saved in 1045 episodes!\tAverage Score: 0.08\n",
      "Episode 1047\t Average Score: 0.08\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 1047 episodes!\tAverage Score: 0.08\n",
      "Episode 1048\t Average Score: 0.09\t max score: 0.30\t min score: 0.19\n",
      "Environment saved in 1048 episodes!\tAverage Score: 0.09\n",
      "Episode 1049\t Average Score: 0.09\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1049 episodes!\tAverage Score: 0.09\n",
      "Episode 1050\tAverage Score: 0.099\t max score: 0.30\t min score: 0.19\n",
      "\n",
      "Environment saved in 1050 episodes!\tAverage Score: 0.09\n",
      "Episode 1052\t Average Score: 0.09\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1052 episodes!\tAverage Score: 0.09\n",
      "Episode 1053\t Average Score: 0.09\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1053 episodes!\tAverage Score: 0.09\n",
      "Episode 1054\t Average Score: 0.09\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1054 episodes!\tAverage Score: 0.09\n",
      "Episode 1055\t Average Score: 0.10\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 1055 episodes!\tAverage Score: 0.10\n",
      "Episode 1059\t Average Score: 0.10\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 1059 episodes!\tAverage Score: 0.10\n",
      "Episode 1061\t Average Score: 0.10\t max score: 0.20\t min score: 0.191\n",
      "Environment saved in 1061 episodes!\tAverage Score: 0.10\n",
      "Episode 1062\t Average Score: 0.10\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1062 episodes!\tAverage Score: 0.10\n",
      "Episode 1063\t Average Score: 0.10\t max score: 0.30\t min score: 0.19\n",
      "Environment saved in 1063 episodes!\tAverage Score: 0.10\n",
      "Episode 1064\t Average Score: 0.10\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1064 episodes!\tAverage Score: 0.10\n",
      "Episode 1065\t Average Score: 0.11\t max score: 0.70\t min score: 0.49\n",
      "Environment saved in 1065 episodes!\tAverage Score: 0.11\n",
      "Episode 1066\t Average Score: 0.11\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1066 episodes!\tAverage Score: 0.11\n",
      "Episode 1068\t Average Score: 0.11\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 1068 episodes!\tAverage Score: 0.11\n",
      "Episode 1069\t Average Score: 0.11\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1069 episodes!\tAverage Score: 0.11\n",
      "Episode 1070\t Average Score: 0.11\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1070 episodes!\tAverage Score: 0.11\n",
      "Episode 1071\t Average Score: 0.12\t max score: 0.40\t min score: 0.29\n",
      "Environment saved in 1071 episodes!\tAverage Score: 0.12\n",
      "Episode 1072\t Average Score: 0.12\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1072 episodes!\tAverage Score: 0.12\n",
      "Episode 1073\t Average Score: 0.12\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1073 episodes!\tAverage Score: 0.12\n",
      "Episode 1079\t Average Score: 0.12\t max score: 0.30\t min score: 0.291\n",
      "Environment saved in 1079 episodes!\tAverage Score: 0.12\n",
      "Episode 1080\t Average Score: 0.12\t max score: 0.60\t min score: 0.49\n",
      "Environment saved in 1080 episodes!\tAverage Score: 0.12\n",
      "Episode 1081\t Average Score: 0.13\t max score: 0.30\t min score: 0.19\n",
      "Environment saved in 1081 episodes!\tAverage Score: 0.13\n",
      "Episode 1082\t Average Score: 0.13\t max score: 0.50\t min score: 0.39\n",
      "Environment saved in 1082 episodes!\tAverage Score: 0.13\n",
      "Episode 1083\t Average Score: 0.13\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1083 episodes!\tAverage Score: 0.13\n",
      "Episode 1084\t Average Score: 0.14\t max score: 0.90\t min score: 0.89\n",
      "Environment saved in 1084 episodes!\tAverage Score: 0.14\n",
      "Episode 1085\t Average Score: 0.16\t max score: 1.79\t min score: 1.70\n",
      "Environment saved in 1085 episodes!\tAverage Score: 0.16\n",
      "Episode 1087\t Average Score: 0.16\t max score: 0.20\t min score: 0.091\n",
      "Environment saved in 1087 episodes!\tAverage Score: 0.16\n",
      "Episode 1088\t Average Score: 0.16\t max score: 0.70\t min score: 0.69\n",
      "Environment saved in 1088 episodes!\tAverage Score: 0.16\n",
      "Episode 1089\t Average Score: 0.17\t max score: 1.00\t min score: 0.89\n",
      "Environment saved in 1089 episodes!\tAverage Score: 0.17\n",
      "Episode 1090\t Average Score: 0.18\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1090 episodes!\tAverage Score: 0.18\n",
      "Episode 1091\t Average Score: 0.18\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1091 episodes!\tAverage Score: 0.18\n",
      "Episode 1092\t Average Score: 0.18\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1092 episodes!\tAverage Score: 0.18\n",
      "Episode 1095\t Average Score: 0.18\t max score: 0.10\t min score: 0.091\n",
      "Environment saved in 1095 episodes!\tAverage Score: 0.18\n",
      "Episode 1096\t Average Score: 0.18\t max score: 0.50\t min score: 0.39\n",
      "Environment saved in 1096 episodes!\tAverage Score: 0.18\n",
      "Episode 1097\t Average Score: 0.19\t max score: 0.90\t min score: 0.89\n",
      "Environment saved in 1097 episodes!\tAverage Score: 0.19\n",
      "Episode 1099\t Average Score: 0.20\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1099 episodes!\tAverage Score: 0.20\n",
      "Episode 1100\tAverage Score: 0.200\t max score: 0.50\t min score: 0.49\n",
      "\n",
      "Environment saved in 1100 episodes!\tAverage Score: 0.20\n",
      "Episode 1101\t Average Score: 0.20\t max score: 0.59\t min score: 0.50\n",
      "Environment saved in 1101 episodes!\tAverage Score: 0.20\n",
      "Episode 1102\t Average Score: 0.21\t max score: 0.30\t min score: 0.19\n",
      "Environment saved in 1102 episodes!\tAverage Score: 0.21\n",
      "Episode 1104\t Average Score: 0.21\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1104 episodes!\tAverage Score: 0.21\n",
      "Episode 1105\t Average Score: 0.21\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 1105 episodes!\tAverage Score: 0.21\n",
      "Episode 1109\t Average Score: 0.21\t max score: 0.50\t min score: 0.491\n",
      "Environment saved in 1109 episodes!\tAverage Score: 0.21\n",
      "Episode 1110\t Average Score: 0.22\t max score: 0.90\t min score: 0.89\n",
      "Environment saved in 1110 episodes!\tAverage Score: 0.22\n",
      "Episode 1111\t Average Score: 0.23\t max score: 0.50\t min score: 0.39\n",
      "Environment saved in 1111 episodes!\tAverage Score: 0.23\n",
      "Episode 1113\t Average Score: 0.23\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1113 episodes!\tAverage Score: 0.23\n",
      "Episode 1114\t Average Score: 0.24\t max score: 1.29\t min score: 1.20\n",
      "Environment saved in 1114 episodes!\tAverage Score: 0.24\n",
      "Episode 1117\t Average Score: 0.25\t max score: 1.20\t min score: 1.091\n",
      "Environment saved in 1117 episodes!\tAverage Score: 0.25\n",
      "Episode 1118\t Average Score: 0.26\t max score: 1.10\t min score: 0.99\n",
      "Environment saved in 1118 episodes!\tAverage Score: 0.26\n",
      "Episode 1119\t Average Score: 0.26\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 1119 episodes!\tAverage Score: 0.26\n",
      "Episode 1122\t Average Score: 0.26\t max score: 0.30\t min score: 0.291\n",
      "Environment saved in 1122 episodes!\tAverage Score: 0.26\n",
      "Episode 1124\t Average Score: 0.26\t max score: 0.40\t min score: 0.391\n",
      "Environment saved in 1124 episodes!\tAverage Score: 0.26\n",
      "Episode 1125\t Average Score: 0.27\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1125 episodes!\tAverage Score: 0.27\n",
      "Episode 1126\t Average Score: 0.27\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1126 episodes!\tAverage Score: 0.27\n",
      "Episode 1127\t Average Score: 0.27\t max score: 0.60\t min score: 0.59\n",
      "Environment saved in 1127 episodes!\tAverage Score: 0.27\n",
      "Episode 1129\t Average Score: 0.27\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1129 episodes!\tAverage Score: 0.27\n",
      "Episode 1130\t Average Score: 0.27\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 1130 episodes!\tAverage Score: 0.27\n",
      "Episode 1133\t Average Score: 0.28\t max score: 0.70\t min score: 0.691\n",
      "Environment saved in 1133 episodes!\tAverage Score: 0.28\n",
      "Episode 1134\t Average Score: 0.28\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1134 episodes!\tAverage Score: 0.28\n",
      "Episode 1135\t Average Score: 0.28\t max score: 0.40\t min score: 0.29\n",
      "Environment saved in 1135 episodes!\tAverage Score: 0.28\n",
      "Episode 1138\t Average Score: 0.29\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1138 episodes!\tAverage Score: 0.29\n",
      "Episode 1139\t Average Score: 0.29\t max score: 0.30\t min score: 0.29\n",
      "Environment saved in 1139 episodes!\tAverage Score: 0.29\n",
      "Episode 1140\t Average Score: 0.29\t max score: 0.19\t min score: 0.10\n",
      "Environment saved in 1140 episodes!\tAverage Score: 0.29\n",
      "Episode 1141\t Average Score: 0.29\t max score: 0.30\t min score: 0.29\n",
      "Environment saved in 1141 episodes!\tAverage Score: 0.29\n",
      "Episode 1145\t Average Score: 0.29\t max score: 0.80\t min score: 0.79\n",
      "Environment saved in 1145 episodes!\tAverage Score: 0.29\n",
      "Episode 1148\t Average Score: 0.30\t max score: 1.40\t min score: 1.391\n",
      "Environment saved in 1148 episodes!\tAverage Score: 0.30\n",
      "Episode 1150\tAverage Score: 0.300\t max score: 0.10\t min score: 0.09\n",
      "Episode 1159\t Average Score: 0.31\t max score: 0.80\t min score: 0.791\n",
      "Environment saved in 1159 episodes!\tAverage Score: 0.31\n",
      "Episode 1160\t Average Score: 0.31\t max score: 0.50\t min score: 0.39\n",
      "Environment saved in 1160 episodes!\tAverage Score: 0.31\n",
      "Episode 1161\t Average Score: 0.32\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1161 episodes!\tAverage Score: 0.32\n",
      "Episode 1177\t Average Score: 0.32\t max score: 0.19\t min score: 0.101\n",
      "Environment saved in 1177 episodes!\tAverage Score: 0.32\n",
      "Episode 1178\t Average Score: 0.32\t max score: 0.20\t min score: 0.09\n",
      "Environment saved in 1178 episodes!\tAverage Score: 0.32\n",
      "Episode 1200\tAverage Score: 0.277\t max score: 0.10\t min score: 0.091\n",
      "Episode 1250\tAverage Score: 0.300\t max score: 0.19\t min score: 0.101\n",
      "Episode 1265\t Average Score: 0.32\t max score: 2.00\t min score: 1.991\n",
      "Environment saved in 1265 episodes!\tAverage Score: 0.32\n",
      "Episode 1266\t Average Score: 0.32\t max score: 0.90\t min score: 0.89\n",
      "Environment saved in 1266 episodes!\tAverage Score: 0.32\n",
      "Episode 1267\t Average Score: 0.32\t max score: 0.30\t min score: 0.29\n",
      "Environment saved in 1267 episodes!\tAverage Score: 0.32\n",
      "Episode 1268\t Average Score: 0.32\t max score: 0.20\t min score: -0.01\n",
      "Environment saved in 1268 episodes!\tAverage Score: 0.32\n",
      "Episode 1271\t Average Score: 0.35\t max score: 2.29\t min score: 2.201\n",
      "Environment saved in 1271 episodes!\tAverage Score: 0.35\n",
      "Episode 1275\t Average Score: 0.35\t max score: 0.50\t min score: 0.391\n",
      "Environment saved in 1275 episodes!\tAverage Score: 0.35\n",
      "Episode 1281\t Average Score: 0.35\t max score: 0.90\t min score: 0.891\n",
      "Environment saved in 1281 episodes!\tAverage Score: 0.35\n",
      "Episode 1282\t Average Score: 0.35\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1282 episodes!\tAverage Score: 0.35\n",
      "Episode 1283\t Average Score: 0.36\t max score: 0.39\t min score: 0.30\n",
      "Environment saved in 1283 episodes!\tAverage Score: 0.36\n",
      "Episode 1284\t Average Score: 0.36\t max score: 1.00\t min score: 0.89\n",
      "Environment saved in 1284 episodes!\tAverage Score: 0.36\n",
      "Episode 1285\t Average Score: 0.37\t max score: 0.80\t min score: 0.79\n",
      "Environment saved in 1285 episodes!\tAverage Score: 0.37\n",
      "Episode 1286\t Average Score: 0.37\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1286 episodes!\tAverage Score: 0.37\n",
      "Episode 1289\t Average Score: 0.37\t max score: 0.30\t min score: 0.291\n",
      "Environment saved in 1289 episodes!\tAverage Score: 0.37\n",
      "Episode 1292\t Average Score: 0.38\t max score: 1.20\t min score: 0.99\n",
      "Environment saved in 1292 episodes!\tAverage Score: 0.38\n",
      "Episode 1293\t Average Score: 0.38\t max score: 0.10\t min score: -0.01\n",
      "Environment saved in 1293 episodes!\tAverage Score: 0.38\n",
      "Episode 1294\t Average Score: 0.38\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1294 episodes!\tAverage Score: 0.38\n",
      "Episode 1297\t Average Score: 0.39\t max score: 0.60\t min score: 0.59\n",
      "Environment saved in 1297 episodes!\tAverage Score: 0.39\n",
      "Episode 1300\tAverage Score: 0.388\t max score: 0.00\t min score: -0.01\n",
      "Episode 1350\tAverage Score: 0.366\t max score: 0.50\t min score: 0.391\n",
      "Episode 1400\tAverage Score: 0.344\t max score: 0.50\t min score: 0.491\n",
      "Episode 1413\t Average Score: 0.39\t max score: 1.70\t min score: 1.691\n",
      "Environment saved in 1413 episodes!\tAverage Score: 0.39\n",
      "Episode 1423\t Average Score: 0.39\t max score: 0.70\t min score: 0.591\n",
      "Environment saved in 1423 episodes!\tAverage Score: 0.39\n",
      "Episode 1424\t Average Score: 0.39\t max score: 0.40\t min score: 0.39\n",
      "Environment saved in 1424 episodes!\tAverage Score: 0.39\n",
      "Episode 1450\tAverage Score: 0.366\t max score: 0.19\t min score: 0.101\n",
      "Episode 1500\tAverage Score: 0.299\t max score: 0.10\t min score: 0.091\n",
      "Episode 1550\tAverage Score: 0.300\t max score: 0.10\t min score: 0.091\n",
      "Episode 1600\tAverage Score: 0.366\t max score: 0.20\t min score: 0.181\n",
      "Episode 1650\tAverage Score: 0.344\t max score: 0.10\t min score: 0.091\n",
      "Episode 1696\t Average Score: 0.39\t max score: 0.60\t min score: 0.491\n",
      "Environment saved in 1696 episodes!\tAverage Score: 0.39\n",
      "Episode 1697\t Average Score: 0.40\t max score: 1.20\t min score: 1.09\n",
      "Environment saved in 1697 episodes!\tAverage Score: 0.40\n",
      "Episode 1698\t Average Score: 0.41\t max score: 0.60\t min score: 0.59\n",
      "Environment saved in 1698 episodes!\tAverage Score: 0.41\n",
      "Episode 1700\tAverage Score: 0.411\t max score: 0.20\t min score: 0.191\n",
      "Episode 1703\t Average Score: 0.41\t max score: 0.70\t min score: 0.69\n",
      "Environment saved in 1703 episodes!\tAverage Score: 0.41\n",
      "Episode 1714\t Average Score: 0.41\t max score: 0.80\t min score: 0.691\n",
      "Environment saved in 1714 episodes!\tAverage Score: 0.41\n",
      "Episode 1715\t Average Score: 0.41\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1715 episodes!\tAverage Score: 0.41\n",
      "Episode 1716\t Average Score: 0.42\t max score: 1.10\t min score: 1.09\n",
      "Environment saved in 1716 episodes!\tAverage Score: 0.42\n",
      "Episode 1717\t Average Score: 0.42\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1717 episodes!\tAverage Score: 0.42\n",
      "Episode 1744\t Average Score: 0.43\t max score: 1.40\t min score: 1.291\n",
      "Environment saved in 1744 episodes!\tAverage Score: 0.43\n",
      "Episode 1745\t Average Score: 0.43\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1745 episodes!\tAverage Score: 0.43\n",
      "Episode 1746\t Average Score: 0.44\t max score: 1.10\t min score: 1.09\n",
      "Environment saved in 1746 episodes!\tAverage Score: 0.44\n",
      "Episode 1747\t Average Score: 0.44\t max score: 1.10\t min score: 0.99\n",
      "Environment saved in 1747 episodes!\tAverage Score: 0.44\n",
      "Episode 1750\tAverage Score: 0.444\t max score: 0.10\t min score: 0.091\n",
      "Episode 1754\t Average Score: 0.45\t max score: 1.70\t min score: 1.591\n",
      "Environment saved in 1754 episodes!\tAverage Score: 0.45\n",
      "Episode 1756\t Average Score: 0.45\t max score: 0.30\t min score: 0.19\n",
      "Environment saved in 1756 episodes!\tAverage Score: 0.45\n",
      "Episode 1759\t Average Score: 0.45\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1759 episodes!\tAverage Score: 0.45\n",
      "Episode 1760\t Average Score: 0.45\t max score: 0.30\t min score: 0.29\n",
      "Environment saved in 1760 episodes!\tAverage Score: 0.45\n",
      "Episode 1773\t Average Score: 0.45\t max score: 0.70\t min score: 0.691\n",
      "Environment saved in 1773 episodes!\tAverage Score: 0.45\n",
      "Episode 1780\t Average Score: 0.46\t max score: 1.10\t min score: 1.09\n",
      "Environment saved in 1780 episodes!\tAverage Score: 0.46\n",
      "Episode 1781\t Average Score: 0.48\t max score: 2.10\t min score: 1.99\n",
      "Environment saved in 1781 episodes!\tAverage Score: 0.48\n",
      "Episode 1800\tAverage Score: 0.433\t max score: 0.20\t min score: -0.01\n",
      "Episode 1850\tAverage Score: 0.400\t max score: 1.20\t min score: 1.091\n",
      "Episode 1900\tAverage Score: 0.433\t max score: 0.30\t min score: 0.291\n",
      "Episode 1937\t Average Score: 0.48\t max score: 0.70\t min score: 0.691\n",
      "Environment saved in 1937 episodes!\tAverage Score: 0.48\n",
      "Episode 1938\t Average Score: 0.50\t max score: 2.60\t min score: 2.60\n",
      "Environment saved in 1938 episodes!\tAverage Score: 0.50\n",
      "Episode 1943\t Average Score: 0.52\t max score: 2.60\t min score: 2.601\n",
      "Environment saved in 1943 episodes!\tAverage Score: 0.52\n",
      "Episode 1945\t Average Score: 0.53\t max score: 1.40\t min score: 1.391\n",
      "Environment saved in 1945 episodes!\tAverage Score: 0.53\n",
      "Episode 1946\t Average Score: 0.53\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1946 episodes!\tAverage Score: 0.53\n",
      "Episode 1950\tAverage Score: 0.511\t max score: 0.29\t min score: 0.201\n",
      "Episode 1958\t Average Score: 0.54\t max score: 1.90\t min score: 1.89\n",
      "Environment saved in 1958 episodes!\tAverage Score: 0.54\n",
      "Episode 1959\t Average Score: 0.54\t max score: 0.50\t min score: 0.39\n",
      "Environment saved in 1959 episodes!\tAverage Score: 0.54\n",
      "Episode 1960\t Average Score: 0.54\t max score: 0.49\t min score: 0.40\n",
      "Environment saved in 1960 episodes!\tAverage Score: 0.54\n",
      "Episode 1961\t Average Score: 0.54\t max score: 0.30\t min score: 0.19\n",
      "Environment saved in 1961 episodes!\tAverage Score: 0.54\n",
      "Episode 1962\t Average Score: 0.55\t max score: 0.50\t min score: 0.49\n",
      "Environment saved in 1962 episodes!\tAverage Score: 0.55\n",
      "Episode 1974\t Average Score: 0.55\t max score: 1.80\t min score: 1.791\n",
      "Environment saved in 1974 episodes!\tAverage Score: 0.55\n",
      "Episode 1977\t Average Score: 0.55\t max score: 1.40\t min score: 1.391\n",
      "Environment saved in 1977 episodes!\tAverage Score: 0.55\n",
      "Episode 1978\t Average Score: 0.57\t max score: 1.79\t min score: 1.70\n",
      "Environment saved in 1978 episodes!\tAverage Score: 0.57\n",
      "Episode 1979\t Average Score: 0.58\t max score: 1.00\t min score: 0.89\n",
      "Environment saved in 1979 episodes!\tAverage Score: 0.58\n",
      "Episode 1980\t Average Score: 0.58\t max score: 0.10\t min score: 0.09\n",
      "Environment saved in 1980 episodes!\tAverage Score: 0.58\n",
      "Episode 1981\t Average Score: 0.59\t max score: 0.60\t min score: 0.59\n",
      "Environment saved in 1981 episodes!\tAverage Score: 0.59\n",
      "Episode 1982\t Average Score: 0.59\t max score: 0.60\t min score: 0.49\n",
      "Environment saved in 1982 episodes!\tAverage Score: 0.59\n",
      "Episode 1986\t Average Score: 0.61\t max score: 2.60\t min score: 2.59\n",
      "Environment saved in 1986 episodes!\tAverage Score: 0.61\n",
      "Episode 1987\t Average Score: 0.62\t max score: 1.90\t min score: 1.89\n",
      "Environment saved in 1987 episodes!\tAverage Score: 0.62\n",
      "Episode 1989\t Average Score: 0.63\t max score: 2.60\t min score: 2.601\n",
      "Environment saved in 1989 episodes!\tAverage Score: 0.63\n",
      "Episode 1990\t Average Score: 0.63\t max score: 0.20\t min score: 0.19\n",
      "Environment saved in 1990 episodes!\tAverage Score: 0.63\n",
      "Episode 2000\tAverage Score: 0.588\t max score: 0.40\t min score: 0.391\n"
     ]
    }
   ],
   "source": [
    "scores_window, scores_array ,mean_scores= train_DDPG(agent,episodes = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad940d0668>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_array)), scores_array)\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
